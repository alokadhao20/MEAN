Normalization versus Denormalization

Normalization is dividing up data into multiple
collections with references between collections.

Denormalization is the opposite of normalization: embedding all of the data in a single
document. Instead of documents containing references to one definitive copy of the
data, many documents may have copies of the data.

Comparison of embedding/Denormalization versus references/Normalization

Embedding is better for... 					References are better for...
Small subdocuments 							Large subdocuments
Data that does not change 					regularly Volatile data
When eventual consistency is acceptable 	When immediate consistency is necessary
Documents that grow by a small amount 		Documents that grow a large amount
Data that you’ll often need to        		Data that you’ll often exclude from the results
 perform a second query to fetch 
Fast reads 									Fast writes

Suppose we had a users collection. Here are some example fields we might have and
whether or not they should be embedded:

Account preferences
They are only relevant to this user document, and will probably be exposed with
other user information in this document. Account preferences should generally be
embedded.

Recent activity
This one depends on how much recent activity grows and changes. If it is a fixedsize
field (last 10 things), it might be useful to embed.

Friends
Generally this should not be embedded, or at least not fully. See the section below
on advice on social networking.

------------------------------------
Cardinality
Cardinality is how many references a collection has to another collection. Common
relationships are
 one-to-one,
 one-to-many,
 or many-to-many.

For example, suppose we
had a blog application. 

Each post has a title, so that’s a one-to-one relationship. 

Each author has many posts, so that’s a one-to-many relationship.
 posts have many tags

tags refer to many posts, so that’s a many-to-many relationship.

When using MongoDB, it can be conceptually useful to split “many” into subcategories:
“many” and “few.” For example, 

you might have a one-to-few cardinality between
authors and posts: each author only writes a few posts. 

You might have many-to-few
relation between blog posts and tags: your probably have many more blog posts than
you have tags.

 However, you’d have a one-to-many relationship between blog posts and
comments: each post has many comments.

When you’ve determined few versus many relations, it can help you decide what to
embed versus what to reference. Generally, “few” relationships will work better with
embedding, and “many” relationships will work better as references.


-----------------------------
Dealing with the Wil Wheaton effect
Regardless of which strategy you use, embedding only works with a limited number of
subdocuments or references. If you have celebrity users, they may overflow any document
that you’re storing followers in. The typical way of compensating this is to have a
“continuation” document, if necessary. For example, you might have:
> db.users.find({"username" : "wil"})
{
	"_id" : ObjectId("51252871d86041c7dca8191a"),
	"username" : "wil",
	"email" : "wil@example.com",
	"tbc" : [
		ObjectId("512528ced86041c7dca8191e"),
		ObjectId("5126510dd86041c7dca81924")
	]
	"followers" : [
	ObjectId("512528a0d86041c7dca8191b"),
	ObjectId("512528a2d86041c7dca8191c"),
	ObjectId("512528a3d86041c7dca8191d"),
	...
	]
}
{
	"_id" : ObjectId("512528ced86041c7dca8191e"),
	"followers" : [
		ObjectId("512528f1d86041c7dca8191f"),
		ObjectId("512528f6d86041c7dca81920"),
		ObjectId("512528f8d86041c7dca81921"),
		...
	]
}
{
	"_id" : ObjectId("5126510dd86041c7dca81924"),
	"followers" : [
		ObjectId("512673e1d86041c7dca81925"),
		ObjectId("512650efd86041c7dca81922"),
		ObjectId("512650fdd86041c7dca81923"),
		...
	]
}

Then add application logic to support fetching the documents in the “to be continued”
("tbc") array.
----------------------

Optimizations for Data Manipulation
Optimizing reads generally involves having the correct
indexes and returning as much of the information as possible in a single document.
Optimizing writes usually involves minimizing the number of indexes you have and
making updates as efficient as possible.

Optimizing for Document Growth
If you’re going to need to update data, determine whether or not your documents are
going to grow and by how much. If it is by a predictable amount, manually padding
your documents will prevent moves, making writes faster. Check your padding factor:
if it is about 1.2 or greater, consider using manual padding.

	looked like this:
{
"_id" : ObjectId(),
"restaurant" : "Le Cirque",
"review" : "Hamburgers were overpriced."
"userId" : ObjectId(),
"tags" : []
}
The "tags" field will grow as users add tags, so the application will often have to perform
an update like this:
> db.reviews.update({"_id" : id},
... {"$push" : {"tags" : {"$each" : ["French", "fine dining", "hamburgers"]}}}})
If "tags" generally doesn’t grow to more than 100 bytes, you could manually pad the
document to prevent any unwanted moves. If you leave the document without padding,
moves will definitely occur as "tags" grows. To pad, add a final field to the document
with whatever field name you’d like:
{
"_id" : ObjectId(),
"restaurant" : "Le Cirque",
"review" : "Hamburgers were overpriced."
"userId" : ObjectId(),
"tags" : [],
"garbage" : "........................................................"+
"................................................................"+
"................................................................"
}
You can either do this on insert or, if the document is created with an upsert, use
"$setOnInsert" to create the field when the document is first inserted.
When you update the document, always "$unset" the "garbage" field:

> db.reviews.update({"_id" : id},
... {"$push" : {"tags" : {"$each" : ["French", "fine dining", "hamburgers"]}}},
... "$unset" : {"garbage" : true}})

The "$unset" will remove the "garbage" field if it exists and be a no-op if it does not.
If your document has one field that grows, try to keep is as the last field in the document
(but before "garbage"). It is slightly more efficient for MongoDB not to have to rewrite
fields after "tags" if it grows.

--------------------------

Removing Old Data
Some data is only important for a brief time: after a few weeks or months it is just wasting
storage space. There are three popular options for removing old data:

capped collections,
TTL collections,
dropping collections per time period.


Capped Collections
MongoDB also supports a different type of collection, called
a capped collection, which is created in advance and is fixed in size

capped
collections behave like circular queues: if we’re out of space, the oldest document will
be deleted, and the new one will take its place (see Figure 6-2). This means that capped
collections automatically age-out the oldest documents as new documents are inserted.

Certain operations are not allowed on capped collections.
 Documents cannot be removed
or deleted (aside from the automatic age-out described earlier), and updates that
would cause documents to grow in size are disallowed.

Capped collections cannot be sharded.

Capped collections tend to be useful for logging, although they lack flexibility: you
cannot control when data ages out, other than setting a size when you create the
collection.

Creating Capped Collections
Unlike normal collections, capped collections must be explicitly created before they are
used. To create a capped collection, use the create command. From the shell, this can
be done using createCollection:

> db.createCollection("my_collection", {"capped" : true, "size" : 100000});
{ "ok" : true }

The previous command creates a capped collection, my_collection, that is a fixed size
of 100,000 bytes.

createCollection can also specify a limit on the number of documents in a capped
collection in addition to the limit size:

> db.createCollection("my_collection2",
... {"capped" : true, "size" : 100000, "max" : 100});
{ "ok" : true }

Once a capped collection has been created, it cannot be changed (it must be dropped
and recreated if you wish to change its properties).

Age-out will be based on whichever
limit is reached first: it cannot hold more than "max" documents nor
take up more than "size" space.

creating a capped collection is to convert an existing, regular collection
into a capped collection

-	db.runCommand({"convertToCapped" : "test", "size" : 10000});
{ "ok" : true }

--------------------------

Time-To-Live Indexes(TTL)

When a document
reaches a preconfigured age, it will be deleted. This type of index is useful for caching
problems like session storage.

You can create a TTL index by specifying the expireAfterSecs option in the second
argument to ensureIndex:

> // 24-hour timeout
> db.foo.ensureIndex({"lastUpdated" : 1}, {"expireAfterSecs" : 60*60*24})

This creates a TTL index on the "lastUpdated" field. If a document’s "lastUpdated"
field exists and is a date, the document will be removed once the server time is expir
eAfterSecs seconds ahead of the document’s time.

To prevent an active session from being removed, you can update the "lastUpdated"
field to the current time whenever there is activity. Once "lastUpdated" is 24 hours
old, the document will be removed.

MongoDB sweeps the TTL index once per minute, so you should not depend on tothe-
second granularity.

------------------------

Planning Out Databases and Collections
documents with a similar schema should be kept in the same collection.
if there are
documents that need to be queried or aggregated together

----------

Managing Consistency
When the client sends a request, it will be placed at the end
of its connection’s queue. Any subsequent requests on the connection will occur after
the enqueued operation is processed. Thus, a single connection has a consistent view
of the database and can always read its own writes.

Note that this is a per-connection queue: if we open two shells, we will have two connections
to the database. If we perform an insert in one shell, a subsequent query in the
other shell might not return the inserted document.
 However, within a single shell, if
we query for the document after inserting, the document will be returned. This behavior
can be difficult to duplicate by hand, but on a busy server interleaved inserts and queries
are likely to occur. Often developers run into this when they insert data in one thread
and then check that it was successfully inserted in another. For a moment or two, it looks
like the data was not inserted, and then it suddenly appears.


----------
Basic facts about mongodb
1. Rich Documents .. array in object and its hiererchy
2. PreJoin embedded data
3. No more joines
4. no constraints (primary and foreign key)
5. Atomic operation(document level)
6. No declared schema

------
Goals for Normalization
1. Free database on modification anomalies(update in one document might leave other document unupdated)

WHAT mongodb do not do 
Avoid any bias towards any perticular access pattern

We are equaly biased for all the access patterns 

--------Alternative schema for designing a blog
posts
{
	_id:1,
	title:----,
	body:....,
	author:....,
	date:....
}


comments
{
	_id:3,
	post_id:1
	Author:...,
	authorEmail:...,
	order:...
}
post_id is refrencing post collection _id
order is the the order in which comments are posted on the blog

tags
{
	_id:34,
	tag:...,
	post_id: 1,
}
post_id is refrencing post collection _id

Problem with this data schema is 
1. we need to manually go to post data and get post same with comments and then with tags for 1 blog

Better approach (embedded data)
posts
{
	_id:1,
	title:----,
	body:....,
	author:....,
	date:....
	comments[
			{
				
				Author:...,
				authorEmail:...,
			}],
	tags[
		{
			_id:34,
			tag:...,
			post_id: 1,
		}]	
}

-------------
We dont have transactions in mongodb 
But we do have atomic operation
Atomic operations means when some work is going on to a document .. the work will first compelte before anyone
sees the document

And as the document is already embedded and we are updating comment or tags the document is under some work 
so no one else can see it before it complete

There are 3 approahes to overcome lack of transactions concept in mongodb
1. Restructure your code and work with single doucment as described above (atomic operation document level)
2. Implement in software .. eg. use findAndModify kind of operations which completes the read and write operation
at a same time 
3. Tolerate a bit inconsistency in case o comments or pics or non imp data to update in few second


_____________1:1 relationship_____________
eg. Employee : Resume
	Building: floor plan
	Patient: medical History

As its 1:1 then we can embed it either Employee details in Resume collection or visa vers 

When not to use: 
1. Frequency of accessing Employee is very much and resume is rarely then dont embed, else embed
2. Size of the item, if more that 16 mb then keep seprate collection
3. Atomicity of data, if we cannot withstand any inconsistency for data .. embed it so that Atomicity apply	


____________1:many relationship_____________
city:person

1. person array in each city document wont work for 8 million people
2. {
	name:...,
	city:{
		name: ...,
		area:...
	}
}
In this approach the duplicate data will appear in each person and for 8 million people this 
data duplication is very much 

Best way is to use true linking

person
{
	name:...,
	city:"NYC"
}

city
{
	_id:"NYC",
	area:...,
	.
	.
}

It requires 2 collection and is the best way for one to many

____________if not one:many but its 1:few_______________

Eg. blogPost:comments   like 1 blogPost is having around 10 cmments 

posts
{
	_id:1,
	title:----,
	body:....,
	author:....,
	comments:[{comment1},{comment2}]
}

if few is really few then embed its

____________many:many______________________
Eg. books:authors
	teachers:students

books
{
	_id:12,
	title:...,
	authors:[27,28]
}

authors
{
	_id:27,
	authorName:....,
	books:[12,14]
}

We are keeping refrence of authro in books collection and as well as books refrence in each auther based 
on there _id field 

1. Embedding few to few is also ok if its ok in read performance reasons 
Thing to note here is 

in case of student:teachers model 
if we think to embdded teachers data into students collection 
then there might be a case that there is a teacher which is not having any student 
So this wont work we need to embdded student in teachers if fisible


_______________multikey indexes___________________

The major reason why linking and embddeding works so well in mongodb is because of multikey indexes

two ovious query on student:teachers linking will be
1. list all the students a teacher has 
2. lsit all the teachere a student has  

above 2 can be listed as ans in teacher array in student collection and student array in teachers collection

But If I want all the student that has perticular teacher

So we will search student collection teachers array to check [1,2,3,4,5]  teacher 1,4

we can apply multikey index on the teachers array of student collection

- db.students.ensureIndex({'teachers':1});

then we can query 
- db.students.find({'teachers':{'$all':[1,3]}});


____________benifits of embddeding______
Improve read performance
One round trip to db as and work done
(as data stored in magnetic disk and to get a data the pointer need to seek at start and then scan
	this seek to start is called as Latency: that is High so High latency )
	Once it reach to start then each perticular document comes very quickly that is high bandwidth
	SO if data is embeded in one document then its only one round trip 

	and if the data is in different collection then multiple round trip .. means time will need

____________representing Trees
Product :categories

same as student and teachers 